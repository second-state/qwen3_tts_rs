name: CI

on:
  push:
  pull_request:

jobs:
  build:
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: ubuntu-latest
            name: Linux x86_64
            pip-extra-args: "--index-url https://download.pytorch.org/whl/cpu"
          - os: ubuntu-24.04-arm
            name: Linux ARM64
            pip-extra-args: ""
          - os: macos-latest
            name: macOS ARM64
            pip-extra-args: ""

    runs-on: ${{ matrix.os }}
    name: Build (${{ matrix.name }})

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry and build
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-${{ runner.arch }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-${{ runner.arch }}-cargo-

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install PyTorch
        run: pip install torch==2.10.0 ${{ matrix.pip-extra-args }}

      - name: Install HuggingFace tools
        run: pip install huggingface_hub transformers

      - name: Set library path (Linux)
        if: runner.os == 'Linux'
        run: echo "LD_LIBRARY_PATH=$(python3 -c 'import torch; print(torch.__path__[0])')/lib:$LD_LIBRARY_PATH" >> "$GITHUB_ENV"

      - name: Set library path (macOS)
        if: runner.os == 'macOS'
        run: echo "DYLD_LIBRARY_PATH=$(python3 -c 'import torch; print(torch.__path__[0])')/lib:$DYLD_LIBRARY_PATH" >> "$GITHUB_ENV"

      - name: Build
        env:
          LIBTORCH_USE_PYTORCH: "1"
        run: cargo build --release

      - name: Build examples
        env:
          LIBTORCH_USE_PYTORCH: "1"
        run: cargo build --examples --release

      - name: Cache model weights
        uses: actions/cache@v4
        with:
          path: models/Qwen3-TTS-12Hz-0.6B-CustomVoice
          key: qwen3-tts-0.6b-customvoice-v1

      - name: Download model
        run: huggingface-cli download Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice --local-dir models/Qwen3-TTS-12Hz-0.6B-CustomVoice

      - name: Generate tokenizer.json
        run: |
          python3 -c "
          from transformers import AutoTokenizer
          tok = AutoTokenizer.from_pretrained('models/Qwen3-TTS-12Hz-0.6B-CustomVoice', trust_remote_code=True)
          tok.backend_tokenizer.save('models/Qwen3-TTS-12Hz-0.6B-CustomVoice/tokenizer.json')
          print('Saved tokenizer.json')
          "

      - name: Generate English audio
        env:
          LIBTORCH_USE_PYTORCH: "1"
        run: |
          cargo run --example tts_demo --release -- \
            models/Qwen3-TTS-12Hz-0.6B-CustomVoice \
            "Hello! This is a test of the Qwen3 TTS system running on CI." \
            Vivian \
            english
          mv output.wav output_english.wav

      - name: Generate Chinese audio
        env:
          LIBTORCH_USE_PYTORCH: "1"
        run: |
          cargo run --example tts_demo --release -- \
            models/Qwen3-TTS-12Hz-0.6B-CustomVoice \
            "你好！这是Qwen3语音合成系统的持续集成测试。" \
            Vivian \
            chinese
          mv output.wav output_chinese.wav

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: qwen3-tts-${{ matrix.os }}-${{ runner.arch }}
          path: |
            target/release/examples/tts_demo
            output_english.wav
            output_chinese.wav
